{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZNUnaLNWcWLR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.1.3; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\home\\pycharmprojects\\quickbooks\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q textx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XFGReP-FTrR1"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: '/content/drive/MyDrive/X_ACL/uk'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Home\\Documents\\GitHub\\recursive_morphological_parser\\Morphological_parsers.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Home/Documents/GitHub/recursive_morphological_parser/Morphological_parsers.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39mimport\u001b[39;00m CFG\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Home/Documents/GitHub/recursive_morphological_parser/Morphological_parsers.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerate\u001b[39;00m \u001b[39mimport\u001b[39;00m generate\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Home/Documents/GitHub/recursive_morphological_parser/Morphological_parsers.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m os\u001b[39m.\u001b[39;49mchdir(\u001b[39m\"\u001b[39;49m\u001b[39m/content/drive/MyDrive/X_ACL/uk\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/drive/MyDrive/X_ACL/uk'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from collections import deque\n",
        "import time\n",
        "from nltk import CFG\n",
        "from nltk.parse.generate import generate\n",
        "os.chdir(\"/content/drive/MyDrive/X_ACL/uk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "G1nBOU53rDGK"
      },
      "outputs": [],
      "source": [
        "p_file = \"suffixes_v2.txt\"\n",
        "r_file = \"roots_v2.txt\"\n",
        "s_file = 'prefixes_v2.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqBA0R6IYArn"
      },
      "source": [
        "Explanation of notation\n",
        "\n",
        "*   '+' -- Prefix (p)\n",
        "*   '$' -- Root (r)\n",
        "*   '-' -- Interfix (i)\n",
        "*   '^' -- Suffix (s)\n",
        "*   '*' -- Ending (e)\n",
        "*   '@' -- Postfix (x)\n",
        "\n",
        "*   '&', 'T' -- start and end of the word\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRIoql5oyJBs"
      },
      "source": [
        "# Mophemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "1DwgASzyyKTp"
      },
      "outputs": [],
      "source": [
        "class Morphemes:\n",
        "    def __init__(self, prefixes, roots, interfixes, suffixes, endings, postfixes):\n",
        "        self.morphemes_dict = self.load_morphemes(prefixes, roots, interfixes, suffixes, endings, postfixes) #dict of morphemes {\"prefixes\":{...}, ...}\n",
        "        #all_morphemes in one set\n",
        "        self.morphemes = self.morphemes_dict[\"prefixes\"] | self.morphemes_dict[\"roots\"] | self.morphemes_dict[\"interfixes\"] | self.morphemes_dict[\"suffixes\"] | self.morphemes_dict[\"endings\"] | self.morphemes_dict[\"postfixes\"]\n",
        "\n",
        "    def load_morphemes(self, prefixes, roots, interfixes, suffixes, endings, postfixes):\n",
        "        morphemes_dict = {\n",
        "            \"prefixes\": set([\"+\" + i for i in prefixes]),\n",
        "            \"roots\": set([\"$\" + i for i in roots]),\n",
        "            \"interfixes\": set([\"-\" + i for i in interfixes]),\n",
        "            \"suffixes\": set([\"^\" + i for i in suffixes]),\n",
        "            \"endings\": set([\"*\" + i for i in endings]),\n",
        "            \"postfixes\": set([\"@\" + i for i in postfixes])\n",
        "        }\n",
        "        return morphemes_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17bmarwnTfiV"
      },
      "source": [
        "# BNF parser (fails, inefficient) DO NOT RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txHeEZT2WWyz"
      },
      "outputs": [],
      "source": [
        "from textx import metamodel_from_str\n",
        "class BNF_parser:\n",
        "  def __init__(self, M: Morphemes):\n",
        "    self.M = M\n",
        "    self.grammar = self.generate_grammar() # TextX metamodel 'mm'\n",
        "\n",
        "  def list_to_bnf(self, lst):\n",
        "    return (\"|\").join([\"'\" + i + \"'\" for i in lst])\n",
        "\n",
        "  def generate_grammar(self):\n",
        "    grammar = f\"\"\"\n",
        "    Word:\n",
        "        base=Base\n",
        "        (end=End)?;\n",
        "\n",
        "    Base:\n",
        "        stem=Stem\n",
        "        (xsuffix=XSuffix)?;\n",
        "\n",
        "    Stem:\n",
        "        (xprefix=XPrefix)?\n",
        "        root=SuperRoot;\n",
        "\n",
        "    XPrefix:\n",
        "        prefix=Prefix\n",
        "        (xprefix=XPrefix)?;\n",
        "\n",
        "    SuperRoot:\n",
        "        root=Root\n",
        "        (xroot=XRoot)?;\n",
        "\n",
        "    XRoot:\n",
        "        (interfix=Interfix)?\n",
        "        base=Base;\n",
        "\n",
        "    XSuffix:\n",
        "      suffix=Suffix\n",
        "      (xsuffix=XSuffix)?;\n",
        "\n",
        "    End:\n",
        "      (ending=Ending)? (postfix=Postfix)?;\n",
        "\n",
        "    Prefix:\n",
        "        ({self.list_to_bnf(self.M.morphemes_dict[\"prefixes\"])});\n",
        "\n",
        "    Root:\n",
        "        {self.list_to_bnf(self.M.morphemes_dict[\"roots\"])};\n",
        "\n",
        "    Interfix:\n",
        "        {self.list_to_bnf(self.M.morphemes_dict[\"interfixes\"])};\n",
        "\n",
        "    Ending:\n",
        "      {self.list_to_bnf(self.M.morphemes_dict[\"endings\"])};\n",
        "\n",
        "    Postfix:\n",
        "      {self.list_to_bnf(self.M.morphemes_dict[\"postfixes\"])};\n",
        "\n",
        "    Suffix:\n",
        "        ({self.list_to_bnf(self.M.morphemes_dict[\"suffixes\"])});\n",
        "    \"\"\"\n",
        "    return metamodel_from_str(grammar)\n",
        "\n",
        "  def check_word(self, w):\n",
        "    try:\n",
        "      model = self.grammar.model_from_str(w)\n",
        "      if model:\n",
        "          print(model)\n",
        "          print(f\"{w} is a valid word according to the morphology\")\n",
        "      else:\n",
        "          print(f\"{w} is not a valid word according to the morphology\")\n",
        "    except:\n",
        "      print(f\"{w} is not a valid word according to the morphology\")\n",
        "\n",
        "  def print_model_tree(self, obj, indent=0):\n",
        "    #print(\" \" * indent + f\"<{obj.__class__.__name__}>\")\n",
        "    if obj is None:\n",
        "        pass#print(\" \" * indent + \"None\")\n",
        "    elif isinstance(obj, str):\n",
        "        print(\" \" * indent + \"\" + obj)\n",
        "    elif isinstance(obj, list):\n",
        "        print(\" \" * indent, [i[:-1] for i in obj])\n",
        "    else:\n",
        "        if hasattr(obj, \"__dict__\"):\n",
        "            for key, value in obj.__dict__.items():\n",
        "              if key != \"parent\" and key[0] != \"_\":\n",
        "                print(\" \"*(indent + 3), \"-\", key)\n",
        "                self.print_model_tree(value, indent + 6)\n",
        "\n",
        "  def tree_for_word(self, w):\n",
        "  # Assuming you already have the TextX metamodel 'mm' and the input 'word' \"+ви+пере$земл-е$трус^ити@ся\"\n",
        "    try:\n",
        "      #print(word)\n",
        "      model = self.grammar.model_from_str(w)\n",
        "\n",
        "      return model\n",
        "    except:\n",
        "      #print(\"Such parsing cannot exist\")\n",
        "      return None\n",
        "\n",
        "  def find_combinations(self, w, morphemes):\n",
        "    result = []\n",
        "\n",
        "    def backtrack(start, path):\n",
        "        if start == len(w):\n",
        "            result.append(path[:])\n",
        "            return\n",
        "\n",
        "        for morpheme in morphemes:\n",
        "            if w.startswith(morpheme[1:], start):\n",
        "                path.append(morpheme)\n",
        "                backtrack(start + len(morpheme)-1, path)\n",
        "                path.pop()\n",
        "\n",
        "    backtrack(0, [])\n",
        "    return result\n",
        "\n",
        "  def print_possible_parsings(self, w):\n",
        "    combinations = list({\"\".join(comb)\n",
        "                    for comb\n",
        "                    in self.find_combinations(w, self.M.morphemes)})\n",
        "    print(combinations)\n",
        "    print(len(combinations))\n",
        "\n",
        "\n",
        "    for c in combinations:\n",
        "      print(40*\"-\", c)\n",
        "      self.print_model_tree(self.tree_for_word(c))\n",
        "\n",
        "  def possible_parsings(self, word):\n",
        "    res = []\n",
        "    combinations = list({\"\".join(comb)\n",
        "                    for comb\n",
        "                    in self.find_combinations(word, self.M.morphemes)})\n",
        "    for c in combinations:\n",
        "      t = self.tree_for_word(c)\n",
        "      if t:\n",
        "        res.append(t)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vV7TOEsdAfH"
      },
      "source": [
        "# Linearizing BNF (more efficient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "0OAFEMRuLZiC"
      },
      "outputs": [],
      "source": [
        "#Define your grammar from string\n",
        "#You can define it using other methods, but I only know this xD\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  W -> Base End | Base\n",
        "  Base -> Stem | Stem Suffix\n",
        "  Stem -> Prefix Root | Root\n",
        "  Suffix -> \"s\" | XSuffix\n",
        "  XSuffix -> \"s\" Suffix\n",
        "  Prefix -> \"p\" | XPrefix\n",
        "  XPrefix -> \"p\" Prefix\n",
        "  End -> \"e\" | \"ex\" | \"x\"\n",
        "  Root -> \"r\" | \"r\" XRoot\n",
        "  XRoot -> \"i\" Base | Base  \"\"\")\n",
        "#With this we \"create\" all the possible combinations\n",
        "grammar.productions()\n",
        "\n",
        "#Here you can see all the productions (sentences) with 5 words\n",
        "#created with this grammar\n",
        "possible_sequences = []\n",
        "i = 0\n",
        "for production in generate(grammar, depth = 10):\n",
        "  possible_sequences.append(''.join(production))\n",
        "  i += 1\n",
        "possible_sequences = list(set(possible_sequences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyUa6G9ycLK5"
      },
      "source": [
        "# Mealy Machine Parser (backtracing -- pretty efficient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "zMnzKfDRcO4R"
      },
      "outputs": [],
      "source": [
        "class MealyMachine:\n",
        "    def __init__(self, word, M:Morphemes):\n",
        "        self.word = word\n",
        "        self.transitions = self.fast_parse()\n",
        "        self.initial_state = word[0] + '0'\n",
        "        self.terminal_state = 'T'\n",
        "        self.allowed_paths = {\"&\":{\"p\", \"r\"},\n",
        "                              \"p\":{\"p\", \"r\"},\n",
        "                              \"r\":{\"p\", \"r\", \"T\", \"s\", \"i\", \"e\"},\n",
        "                              \"i\":{\"p\", \"r\"},\n",
        "                              \"s\":{\"s\", \"e\", \"x\", \"T\", \"i\", \"p\"},\n",
        "                              \"e\":{\"x\", \"T\"},\n",
        "                              \"x\":{\"T\"},\n",
        "                              \"T\":{\"T\"}}\n",
        "\n",
        "    def num_letter_2_word(self, num_lettered_word: list):\n",
        "      \"\"\"[\"n1\", \"m2\"] -> nm\"\"\"\n",
        "      return \"\".join([i[0] for i in num_lettered_word])\n",
        "\n",
        "    def morphemes_by_sequence(self, seqs):\n",
        "      \"\"\"перед -> {p, r}\"\"\"\n",
        "      result = set()\n",
        "      if \"+\" + seqs in M.morphemes:\n",
        "        result.add(\"p\")\n",
        "      if \"$\" + seqs in M.morphemes:\n",
        "        result.add(\"r\")\n",
        "      if \"-\" + seqs in M.morphemes:\n",
        "        result.add(\"i\")\n",
        "      if \"^\" + seqs in M.morphemes:\n",
        "        result.add(\"s\")\n",
        "      if \"*\" + seqs in M.morphemes:\n",
        "        result.add(\"e\")\n",
        "      if \"@\" + seqs in M.morphemes:\n",
        "        result.add(\"x\")\n",
        "      return result\n",
        "\n",
        "    def word2matrix(self, word):\n",
        "      letters_to_check = [word[i]+str(i) for i in range(len(word))]\n",
        "      letter_matrix = {}\n",
        "      for i in range(len(letters_to_check)):\n",
        "        letter_matrix[letters_to_check[i]] = list()\n",
        "\n",
        "      indicies_to_check = [0]\n",
        "      while len(indicies_to_check) != 0:\n",
        "        for index in indicies_to_check:\n",
        "          indicies_to_check = []\n",
        "          for i in range(len(letters_to_check)+1):\n",
        "            if index > len(word):\n",
        "              break\n",
        "            combs = self.morphemes_by_sequence(word[index:i])\n",
        "            if combs != set():\n",
        "              combs = ([j + str(i-1) for j in list(combs)])\n",
        "              letter_matrix[letters_to_check[index]] = letter_matrix[letters_to_check[index]]+([(letters_to_check[i-1],combs)])\n",
        "              indicies_to_check.append(i)\n",
        "      sequence = {letters_to_check[i]:letters_to_check[i+1] for i in range(len(letters_to_check) - 1)}\n",
        "      sequence[letters_to_check[-1]] = \"T\"\n",
        "      return letter_matrix, sequence, letters_to_check\n",
        "\n",
        "    def fast_parse(self):\n",
        "      word = self.word\n",
        "      matrix = self.word2matrix(word)\n",
        "      transitions = {}\n",
        "      for letter in matrix[2]:\n",
        "          if matrix[0][letter] == []:\n",
        "            transitions[letter] = {}\n",
        "\n",
        "          for trans in matrix[0][letter]:\n",
        "            temp = dict()\n",
        "            for morph in trans[1]:\n",
        "              temp[morph] = {\"output\": self.num_letter_2_word(matrix[2][int(letter[1:]):int(trans[0][1:])+1]), \"next_state\":matrix[1][trans[0]]}\n",
        "            if letter in transitions:\n",
        "              transitions[letter].update(temp)\n",
        "            else:\n",
        "              transitions[letter] = temp\n",
        "      transitions['T'] = {'/n': {'output': '/n', 'next_state': 'T'}}\n",
        "      return transitions\n",
        "\n",
        "    def get_possible_inputs(self, from_state, to_state):\n",
        "        possible_inputs = []\n",
        "        for symbol, transitions in self.transitions[from_state].items():\n",
        "            if transitions['next_state'] == to_state:\n",
        "                possible_inputs.append(symbol)\n",
        "        return possible_inputs\n",
        "\n",
        "    def backtrace_paths(self, current_state, terminal_state, current_output=\"\", path=[], permitted_transitions = \"&\"):\n",
        "        if current_state == terminal_state:\n",
        "            return [(path + [(current_state, current_output)])]\n",
        "\n",
        "        paths = []\n",
        "        for symbol, transitions in self.transitions[current_state].items():\n",
        "\n",
        "            next_state = transitions['next_state']\n",
        "            output = transitions['output']\n",
        "\n",
        "            possible_inputs = self.get_possible_inputs(current_state, next_state)\n",
        "            for input_symbol in possible_inputs:\n",
        "\n",
        "                #print(permitted_transitions)\n",
        "                x = self.allowed_paths.get(permitted_transitions)\n",
        "                #print(permitted_transitions, input_symbol[0], x)\n",
        "                if input_symbol[0] in x:\n",
        "                  new_path = path + [(current_state, input_symbol, output)]\n",
        "                  new_output = current_output + output\n",
        "                  paths.extend(self.backtrace_paths(next_state, terminal_state, new_output, new_path, permitted_transitions=input_symbol[0]))\n",
        "\n",
        "        return paths\n",
        "\n",
        "    def remove_duplicates(self, list_of_lists):\n",
        "      list_of_tuples = [tuple(inner_list) for inner_list in list_of_lists]\n",
        "      unique_set = set(list_of_tuples)\n",
        "      unique_list_of_lists = [list(unique_tuple) for unique_tuple in unique_set]\n",
        "      return unique_list_of_lists\n",
        "\n",
        "    def all_paths(self):\n",
        "      all_paths = self.backtrace_paths(self.initial_state, self.terminal_state)\n",
        "      #print(all_paths)\n",
        "      return self.remove_duplicates(all_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMoeAqIYmWBc"
      },
      "source": [
        "# Filter using linearized BNF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "v3HDWcLqmbcC"
      },
      "outputs": [],
      "source": [
        "class Filter:\n",
        "  def __init__(self, word, M:Morphemes, morph_seqs):\n",
        "    self.word = word\n",
        "    self.mealy = MealyMachine(word, M)\n",
        "    self.automata = self.mealy.fast_parse()\n",
        "    self.graph = self.automata2graph()\n",
        "\n",
        "  def automata2graph(self):\n",
        "    \"\"\"\n",
        "    { 'з0': {'r2': {'output': 'зем', 'next_state': 'л3'},\n",
        "      'r3': {'output': 'земл', 'next_state': 'е4'}},\n",
        "      'е1': {},\n",
        "      'м2': {},\n",
        "      'л3': {},\n",
        "      'е4': {'i4': {'output': 'е', 'next_state': 'T'},\n",
        "        'e4': {'output': 'е', 'next_state': 'T'}},\n",
        "      'T': {'/n': {'output': '/n', 'next_state': 'T'}}}\n",
        "\n",
        "      ------>\n",
        "\n",
        "    {'з0': {'л3': ['r2'], 'е4': ['r3']},\n",
        "    'е1': {},\n",
        "    'м2': {},\n",
        "    'л3': {},\n",
        "    'е4': {'T': ['i4', 'e4']},\n",
        "    'T': {'T': ['/n']}}\n",
        "    \"\"\"\n",
        "    graph = dict()\n",
        "    for k in self.automata.keys():\n",
        "      temp = dict()\n",
        "      for m in self.automata[k]:\n",
        "        temp[self.automata[k][m]['next_state']] = []\n",
        "      for m in self.automata[k]:\n",
        "        temp[self.automata[k][m]['next_state']].append(m)\n",
        "      graph[k] = (temp)\n",
        "    return(graph)\n",
        "\n",
        "  def find_matching_paths(self, chars):\n",
        "    start = self.word[0] + '0'\n",
        "    queue = deque([(start, [])])\n",
        "    paths = []\n",
        "\n",
        "    while queue:\n",
        "        node, path = queue.popleft()\n",
        "\n",
        "        for neighbor in self.graph[node]:\n",
        "            edges = self.graph[node][neighbor]\n",
        "            for edge in edges:\n",
        "                if ''.join(char for char in edge if char.isalpha()) == chars[len(path)]:\n",
        "                    new_path = path + [neighbor]\n",
        "                    if len(new_path) == len(chars):\n",
        "                        paths.append([start] + new_path)\n",
        "                    else:\n",
        "                        queue.append((neighbor, new_path))\n",
        "\n",
        "    return paths\n",
        "\n",
        "  def inspect(self):\n",
        "    #print(self.graph)\n",
        "    res = []\n",
        "    for chars in possible_sequences:\n",
        "      chars = chars + 'n'\n",
        "      graph = self.graph\n",
        "      start = list(self.graph.keys())[0]\n",
        "      if len(chars) <= len(self.word)+1:\n",
        "        if self.find_matching_paths(chars):\n",
        "          res.append((chars, self.find_matching_paths(chars)))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3eKY3XTUi1F"
      },
      "source": [
        "# !Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tat2_D3FUo8k"
      },
      "source": [
        "## vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "1lLdUO-trWVd"
      },
      "outputs": [],
      "source": [
        "def preprocess_morph(lst):\n",
        "  return [''.join(c for c in i if c.isalnum()) for i in lst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CFMseRgGUp58"
      },
      "outputs": [],
      "source": [
        "prefixes =  ['пере', 'роз', 'за', 'про', 'при', 'ви', 'не', 'до', 'недо', \"воз\", \"перед\", \"по\", \"що\"] \\\n",
        "#+ preprocess_morph([line.strip() for line in open('prefixs_v2.txt', 'r')])\n",
        "roots = ['мор', 'буд', 'зна', 'ход', 'від', 'мов', 'земл', 'дерев', 'жит', 'сел', 'вод', 'зем', 'кра', 'неб', 'міс', 'гір', 'ліс',\n",
        "         'сон', 'дит', 'річ', 'мов', 'світ', 'голос', 'звук', 'тіл', 'серц', 'дух', 'люб', 'слов', 'час', 'рух', 'смаг', 'ліп',\n",
        "         'люб', 'смаж', 'привіт', 'мандр', 'земл', 'трус', 'сп', 'свин', 'жер', 'вір', 'й', 'і', 'д', 'вз',\n",
        "         \"бач\", \"велич\", \"перед\", \"мокр\", \"год\", \"погод\", \"урок\", \"грош\", \"мил\", \"перед\"]\\\n",
        "#+ preprocess_morph([line.strip() for line in open('roots_v2.txt', 'r')])\n",
        "interfixes = [\"о\", \"е\"]\n",
        "suffixes = ['ити', 'ти', \"ть\", 'ати', 'яти', 'ув', 'ство', 'к', 'анн',\"ин\", \"ок\", \"ов\",\"есеньк\", \"ути\"] \\\n",
        "#+ preprocess_morph([line.strip() for line in open('suffixs_v2.txt', 'r')])\n",
        "endings = [\"лю\", \"имо\", \"а\", \"я\", \"е\", \"ило\", \"у\", \"ий\"]\n",
        "postfixes = [\"ся\", \"сь\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "19QUFhSNra0w"
      },
      "outputs": [],
      "source": [
        "M = Morphemes(prefixes, roots, interfixes, suffixes, endings, postfixes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leSrBYaGUqlj"
      },
      "source": [
        "## BNF parser (very slow + errant) DO NOT RUN!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fB_8eL8UkL_",
        "outputId": "f1be7d86-91f0-4404-8b2c-19aaa0620305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<textx:Word instance at 0x7c1360083850>\n",
            "+що$земл-е$трус*у is a valid word according to the morphology\n"
          ]
        }
      ],
      "source": [
        "# B = BNF_parser(M)\n",
        "# B.check_word(\"+що$земл-е$трус*у\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYwBCbXszQoj",
        "outputId": "d6172196-5598-48d5-da8c-c6c898e62a47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['+що$земл*е$трус*у', '+що$земл-е$трус*у']\n",
            "2\n",
            "---------------------------------------- +що$земл*е$трус*у\n",
            "---------------------------------------- +що$земл-е$трус*у\n",
            "    - base\n",
            "          - stem\n",
            "                - xprefix\n",
            "                      - prefix\n",
            "                        +що\n",
            "                      - xprefix\n",
            "                - root\n",
            "                      - root\n",
            "                        $земл\n",
            "                      - xroot\n",
            "                            - interfix\n",
            "                              -е\n",
            "                            - base\n",
            "                                  - stem\n",
            "                                        - xprefix\n",
            "                                        - root\n",
            "                                              - root\n",
            "                                                $трус\n",
            "                                              - xroot\n",
            "                                  - xsuffix\n",
            "          - xsuffix\n",
            "    - end\n",
            "          - ending\n",
            "            *у\n",
            "          - postfix\n"
          ]
        }
      ],
      "source": [
        "# B.print_possible_parsings(\"щоземлетрусу\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AiRlMPVaMQu",
        "outputId": "956221e6-e670-4cb5-fe03-ceed367daa96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Speed: 0.09627984285354614 per word\n"
          ]
        }
      ],
      "source": [
        "#stress testing\n",
        "start = time.time()\n",
        "n = 100\n",
        "for i in range(n):\n",
        "  B = BNF_parser(M)\n",
        "  B.possible_parsings(\"недопереземлетруситися\")\n",
        "print(f\"Speed: {(time.time() - start)/n} per word\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdB7ae85cAzU"
      },
      "source": [
        "## Mealy Machine Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-ZBcPohfMCd",
        "outputId": "091ae8f1-e270-4c0c-935d-91f93e36e450"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('і0', 'r0', 'і'), ('т1', 's2', 'ти'), ('T', 'іти')]]"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mealy = MealyMachine(\"іти\", M)\n",
        "mealy.all_paths()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmpmgnTvsyuf",
        "outputId": "2a0624af-3fca-4141-bb6c-a876e24eeac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Speed: 0.0011097908020019532 per word\n"
          ]
        }
      ],
      "source": [
        "#stress testing\n",
        "start = time.time()\n",
        "n = 100\n",
        "for i in range(n):\n",
        "  mealy = MealyMachine(\"переземлетруситися\", M)\n",
        "  mealy.all_paths()\n",
        "print(f\"Speed: {(time.time() - start)/n} per word\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dex5DoV4qgh1"
      },
      "source": [
        "## Mealy Parser + inspection by linearized BNF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZGj7QW_qmRB",
        "outputId": "cf2a8c20-f671-4801-dfcb-59ed9dd81d7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('rsn', [['п0', 'у5', 'T', 'T']]), ('prsn', [['п0', 'д4', 'у5', 'T', 'T']])]"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f = Filter(\"передути\", M, possible_sequences)\n",
        "f.inspect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-JuNHnLryhR",
        "outputId": "0155e15d-0860-4274-ab17-b75fe0dec3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Speed: 0.0011785221099853515 per word\n"
          ]
        }
      ],
      "source": [
        "#stress testing\n",
        "start = time.time()\n",
        "n = 100\n",
        "for i in range(n):\n",
        "  f = Filter(\"спимо\", M, possible_sequences)\n",
        "  f.inspect()\n",
        "print(f\"Speed: {(time.time() - start)/n} per word\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wRIoql5oyJBs",
        "17bmarwnTfiV",
        "6vV7TOEsdAfH",
        "HyUa6G9ycLK5",
        "Tat2_D3FUo8k",
        "leSrBYaGUqlj",
        "RdB7ae85cAzU",
        "dex5DoV4qgh1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tap-quickbooks",
      "language": "python",
      "name": "tap-quickbooks"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
